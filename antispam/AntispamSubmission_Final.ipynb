{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import json\n",
    "import base64\n",
    "import random\n",
    "from collections import Counter\n",
    "import os.path\n",
    "import imp\n",
    "import gzip\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert2unicode(f):\n",
    "    def tmp(text):\n",
    "        if not isinstance(text, unicode): text = text.decode('utf8')\n",
    "        return f(text)\n",
    "    return tmp\n",
    "\n",
    "def convert2lower(f):\n",
    "    def tmp(text):        \n",
    "        return f(text.lower())\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def html2text_bs(raw_html):\n",
    "    from bs4 import BeautifulSoup\n",
    "    \"\"\"\n",
    "    Тут производится извлечения из html текста\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    \n",
    "    titles = u''\n",
    "    for title in soup.find_all('title'):\n",
    "        titles += title.string + u' ' if title.string else u''\n",
    "        \n",
    "    links = u''\n",
    "    for link in soup.find_all('a'):\n",
    "        links += link.string + u' ' if link.string else u''\n",
    "        \n",
    "    keywords = u''\n",
    "    for key in soup.find_all(\"meta\", attrs={\"name\":\"keywords\"}):\n",
    "        try:\n",
    "            keywords += key['content'] + u' ' if key['content'] else u''\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    [s.extract() for s in soup(['script', 'style'])]\n",
    "    return soup.get_text(), titles, links, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@convert2lower\n",
    "@convert2unicode\n",
    "def easy_tokenizer(text):\n",
    "    word = unicode()\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = unicode()\n",
    "    if word: yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def html2word(raw_html, to_text=html2text_bs, tokenizer=easy_tokenizer):\n",
    "    text, title, links, keywords = to_text(raw_html)\n",
    "    return list(tokenizer(text.lower())), list(tokenizer(title.lower())), \\\n",
    "        list(tokenizer(links.lower())), list(tokenizer(keywords.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file2docs_csv(input_file_name, reparse=False):    \n",
    "    result = []\n",
    "    \n",
    "    if os.path.exists(input_file_name + \".json\") and not reparse:\n",
    "        logging.info(\"File %s.json already exists - load it\" % input_file_name)\n",
    "        result = json.load(open(input_file_name + \".json\", 'rb'))\n",
    "    else:                                \n",
    "        with gzip.open(input_file_name) if input_file_name.endswith('gz') \\\n",
    "            else open(input_file_name)  as input_file:            \n",
    "            headers = input_file.readline()\n",
    "            try:\n",
    "                for i, line in enumerate(input_file):\n",
    "\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    url_id = int(parts[0])                                        \n",
    "                    mark = bool(int(parts[1]))                    \n",
    "                    url = parts[2]\n",
    "                    pageInb64 = parts[3]\n",
    "                    html_data = base64.b64decode(pageInb64)                \n",
    "                    words, title, links, keywords = html2word(html_data)\n",
    "\n",
    "                    result.append( (url_id, mark, url, words, title, links, keywords))  \n",
    "                    if i % 100 == 0: logging.info(\"Complete %04d\" % i)\n",
    "            except:\n",
    "                print i, parts                \n",
    "                raise\n",
    "\n",
    "                    \n",
    "        logging.info(\"Complete %04d\" % i)\n",
    "        logging.info(\"Create json dump %s\" % (input_file_name + \".json\"))\n",
    "        json.dump(result, open(input_file_name + \".json\", 'wb'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import zlib\n",
    "\n",
    "def calc_stats(words, title, links):\n",
    "    words_str = ''.join(words).encode('utf-8')\n",
    "    compress_coeff = sys.getsizeof(zlib.compress(words_str)) * 1.0 /sys.getsizeof(words_str)\n",
    "    \n",
    "    return [len(words), np.mean(map(len, words)), len(title), len(links), compress_coeff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_n_grams(url, window=5):\n",
    "    return set(url[i:i+window] for i in range(0, len(url) - window, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'whistkeruso',\n",
       " u'narod',\n",
       " u'ru',\n",
       " u'prostitutki',\n",
       " u'shlyuhi',\n",
       " u'deshevie',\n",
       " u'moskvi',\n",
       " u'html']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import unquote\n",
    "from urlparse import urlparse\n",
    "import re\n",
    "\n",
    "def parse_url(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    parts = [] \n",
    "    parts.extend(url_parsed.netloc.split('.'))\n",
    "    parts.extend(re.split(r'[-_/.]', url_parsed.path))\n",
    "    return [p for p in parts if p != '']\n",
    "\n",
    "url = u'http://whistkeruso.narod.ru/prostitutki-shlyuhi-deshevie-moskvi.html'\n",
    "parse_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:34:37 INFO:File kaggle/kaggle_train_data_tab.csv.gz.json already exists - load it\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_FILE  = 'kaggle/kaggle_train_data_tab.csv.gz'\n",
    "train_docs = file2docs_csv(TRAIN_DATA_FILE, reparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7044"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:34:49 INFO:File kaggle/kaggle_test_data_tab.csv.gz.json already exists - load it\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_FILE  = 'kaggle/kaggle_test_data_tab.csv.gz'\n",
    "test_docs = file2docs_csv(TEST_DATA_FILE, reparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16039"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles      = {'train': [], 'test': []}\n",
    "urls_parsed = {'train': [], 'test': []}\n",
    "urls_raw    = {'train': [], 'test': []}\n",
    "keywords    = {'train': [], 'test': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = [], []\n",
    "\n",
    "for doc in train_docs:\n",
    "    doc_id, is_spam, url, words_, title, links, keywords_ = doc\n",
    "    \n",
    "    title = [w[:5] for w in title]\n",
    "    keywords_ = [w[:5] for w in keywords_]\n",
    "    \n",
    "    X_train.append(calc_stats(words_, title, links))\n",
    "    \n",
    "    titles['train'].append(' '.join(title))\n",
    "    urls_parsed['train'].append(' '.join(parse_url(url)))\n",
    "    urls_raw['train'].append(url)\n",
    "    keywords['train'].append(' '.join(keywords_))\n",
    "    \n",
    "    Y_train.append(is_spam)\n",
    "    \n",
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "\n",
    "X_test, Y_test = [], []\n",
    "\n",
    "for doc in test_docs:\n",
    "    doc_id, is_spam, url, words_, title, links, keywords_ = doc\n",
    "    \n",
    "    title = [w[:5] for w in title]\n",
    "    keywords_ = [w[:5] for w in keywords_]\n",
    "    \n",
    "    X_test.append(calc_stats(words_, title, links))\n",
    "    \n",
    "    titles['test'].append(' '.join(title))\n",
    "    urls_parsed['test'].append(' '.join(parse_url(url)))\n",
    "    urls_raw['test'].append(url)\n",
    "    keywords['test'].append(' '.join(keywords_))\n",
    "    \n",
    "    doc_ids.append(doc_id)\n",
    "    \n",
    "    # Y_test.append(is_spam)\n",
    "    \n",
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "# vec = CountVectorizer()\n",
    "\n",
    "feature_train = vec.fit_transform(titles['train'])\n",
    "feature_test  = vec.transform(titles['test'])\n",
    "\n",
    "feature_indexes = np.where(feature_train.sum(axis=0) > 10.0)[1]\n",
    "\n",
    "feature_train = feature_train[:, feature_indexes]\n",
    "feature_test  = feature_test[:, feature_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train.toarray()))\n",
    "X_test  = np.hstack((X_test, feature_test.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_all = []\n",
    "\n",
    "for doc in train_docs:\n",
    "    doc_id, is_spam, url, words_, title, links, keywords_ = doc\n",
    "    words_all.append(' '.join(words_))\n",
    "\n",
    "for doc in test_docs:\n",
    "    doc_id, is_spam, url, words_, title, links, keywords_ = doc\n",
    "    words_all.append(' '.join(words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23083, 356061)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "\n",
    "words_all = vec.fit_transform(words_all)\n",
    "words_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 700.0\n",
    "\n",
    "words_most_frequent = np.asarray(words_all.sum(axis=0)).reshape(-1)\n",
    "words_most_frequent = words_most_frequent.argsort()[-int(n):]\n",
    "words_most_frequent.sort()\n",
    "\n",
    "feature_train = words_all[:len(train_docs), words_most_frequent]\n",
    "feature_train = feature_train.sum(axis=1)\n",
    "feature_train = np.asarray(feature_train)\n",
    "\n",
    "feature_test = words_all[len(train_docs):, words_most_frequent]\n",
    "feature_test = feature_test.sum(axis=1)\n",
    "feature_test = np.asarray(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7044, 230), (7044, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, feature_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train))\n",
    "X_test  = np.hstack((X_test, feature_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_train = words_all[:len(train_docs), words_most_frequent]\n",
    "feature_train /= feature_train\n",
    "feature_train = np.nan_to_num(feature_train)\n",
    "feature_train = feature_train.sum(axis=1) / n\n",
    "feature_train = np.asarray(feature_train)\n",
    "\n",
    "feature_test = words_all[len(train_docs):, words_most_frequent]\n",
    "feature_test /= feature_test\n",
    "feature_test = np.nan_to_num(feature_test)\n",
    "feature_test = feature_test.sum(axis=1) / n\n",
    "feature_test = np.asarray(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train))\n",
    "X_test  = np.hstack((X_test, feature_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = TfidfTransformer()\n",
    "\n",
    "vec.fit(words_all[:, words_most_frequent])\n",
    "\n",
    "feature_train = vec.transform(words_all[:len(train_docs), words_most_frequent])\n",
    "feature_test  = vec.transform(words_all[len(train_docs):, words_most_frequent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train.toarray()))\n",
    "X_test  = np.hstack((X_test, feature_test.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del words_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: ngrams independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_ngram_independence(ngrams):\n",
    "    probs = np.asarray(Counter(ngrams).values(), dtype=float) / len(ngrams)\n",
    "    measure = -np.mean(np.log(probs))\n",
    "    return measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_train = []\n",
    "    \n",
    "for doc in train_docs:\n",
    "    doc_id, is_spam, url, words_, title, links, keywords_ = doc\n",
    "    ngr = get_n_grams(' '.join(words_), window=7)\n",
    "    feature_train.append(measure_ngram_independence(ngr))\n",
    "    \n",
    "feature_train = np.asarray(feature_train).reshape(-1, 1)\n",
    "\n",
    "feature_test  = []\n",
    "\n",
    "for doc in test_docs:\n",
    "    doc_id, is_spam, url, words_, title, links, keywords_ = doc\n",
    "    ngr = get_n_grams(' '.join(words_), window=7)\n",
    "    feature_test.append(measure_ngram_independence(ngr))\n",
    "\n",
    "feature_test = np.asarray(feature_test).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train))\n",
    "X_test  = np.hstack((X_test, feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: words in urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "\n",
    "feature_train = vec.fit_transform(urls_parsed['train'])\n",
    "feature_test  = vec.transform(urls_parsed['test'])\n",
    "\n",
    "feature_indexes = np.where(feature_train.sum(axis=0) > 10.0)[1]\n",
    "\n",
    "feature_train = feature_train[:, feature_indexes]\n",
    "feature_test  = feature_test[:, feature_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train.toarray()))\n",
    "X_test  = np.hstack((X_test, feature_test.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: urls ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_ratio(url):    \n",
    "    url_parsed = urlparse(url)\n",
    "    parts = re.split(r'[-_/.]', url_parsed.path)\n",
    "    \n",
    "    parts = np.asarray([len(p) for p in parts if p != ''], dtype=float)\n",
    "    if len(parts) == 0:\n",
    "        return 0\n",
    "    return np.mean(parts / len(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_train = np.asarray(map(url_ratio, urls_raw['train'])).reshape(-1, 1)\n",
    "feature_test  = np.asarray(map(url_ratio, urls_raw['test'])).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train))\n",
    "X_test  = np.hstack((X_test, feature_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_ratio(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    parts = re.split(r'[-_/.]', url_parsed.path)\n",
    "    return len(url_parsed.netloc) / float(len(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_train = np.asarray(map(url_ratio, urls_raw['train'])).reshape(-1, 1)\n",
    "feature_test  = np.asarray(map(url_ratio, urls_raw['test'])).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train))\n",
    "X_test  = np.hstack((X_test, feature_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_ratio(url):    \n",
    "    url_parsed = urlparse(url)\n",
    "    parts = re.split(r'[/]', url_parsed.path)[-1]\n",
    "    return len(re.split(r'[.-_]', parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_train = np.asarray(map(url_ratio, urls_raw['train'])).reshape(-1, 1)\n",
    "feature_test  = np.asarray(map(url_ratio, urls_raw['test'])).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train))\n",
    "X_test  = np.hstack((X_test, feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_url_f(url):\n",
    "    url_parsed = urlparse(url)\n",
    "    segments = [s for s in url_parsed.path.split('/') if s != '']\n",
    "    q_params = [q for q in url_parsed.query.split('&') if q != '']\n",
    "    return segments, q_params\n",
    "\n",
    "\n",
    "def extract_features_from_url(url):\n",
    "    segments, q_params = parse_url_f(url)\n",
    "    features = []\n",
    "\n",
    "    if q_params:\n",
    "        features += ['param_name:{}'.format(param.split('=')[0]) for param in q_params]\n",
    "        features += ['param:{}'.format(param) for param in q_params]\n",
    "\n",
    "    if len(segments) == 0:\n",
    "        return features\n",
    "\n",
    "    features.append('segments:{}'.format(len(segments)))\n",
    "\n",
    "    categories_templates = [\n",
    "        'segment_name_{}:{}',             # name of seg\n",
    "        'segment_[0-9]_{}:1',             # seg consists of digits\n",
    "        'segment_substr[0-9]_{}:1',       # seg has pattern <str><digits><str>\n",
    "        'segment_ext_{}:{}',              # seg has an extension\n",
    "        'segment_ext_substr[0-9]_{}:{}',  # seg has pattern and an extension\n",
    "        'segment_len_{}:{}',              # length of seg\n",
    "\n",
    "        'wiki_lines_{}:{}',\n",
    "        'wiki_underlines_{}:{}',\n",
    "        'wiki_spaces_{}:{}',\n",
    "        'wiki_all_spaces_{}:{}',\n",
    "    ]\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        features.append(categories_templates[0].format(i, seg))\n",
    "\n",
    "        seg_name, seg_ext = os.path.splitext(seg)\n",
    "        seg_ext = seg_ext[1:]            # remove dot in extension\n",
    "\n",
    "        if re.search(r'^(\\d)+$', seg_name):\n",
    "            features.append(categories_templates[1].format(i))\n",
    "\n",
    "        # pattern_1 = re.search(r'^([^\\d]*)(\\d+)([^\\d]+)$', seg_name)\n",
    "        # pattern_2 = re.search(r'^([^\\d]+)(\\d+)([^\\d]*)$', seg_name)\n",
    "        pattern = re.search(r'[^\\d]+\\d+[^\\d]+$', seg_name)\n",
    "\n",
    "        if pattern:\n",
    "            features.append(categories_templates[2].format(i))\n",
    "        if seg_ext:\n",
    "            features.append(categories_templates[3].format(i, seg_ext))\n",
    "        if pattern and seg_ext:\n",
    "            features.append(categories_templates[4].format(i, seg_ext))\n",
    "        features.append(categories_templates[5].format(i, len(seg)))\n",
    "\n",
    "        counts_lines = seg_name.count('-')\n",
    "        if counts_lines:\n",
    "            features.append(categories_templates[6].format(i, counts_lines))\n",
    "        \n",
    "        counts_underlines = seg_name.count('_')\n",
    "        if counts_underlines:\n",
    "            features.append(categories_templates[7].format(i, counts_underlines))\n",
    "\n",
    "        counts_spaces = seg_name.count(' ')\n",
    "        if counts_spaces:\n",
    "            features.append(categories_templates[8].format(i, counts_spaces))\n",
    "\n",
    "        if counts_underlines + counts_spaces:\n",
    "            features.append(categories_templates[9].format(i, counts_lines + counts_underlines + counts_spaces))\n",
    "\n",
    "    return features\n",
    "\n",
    "def choose_features(features, threshold):\n",
    "    return [f for f in features if f[1] > threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segments_train = map(extract_features_from_url, urls_raw['train'])\n",
    "segments_test  = map(extract_features_from_url, urls_raw['test'])\n",
    "\n",
    "alpha = .05\n",
    "segments = []\n",
    "[segments.extend(s) for s in segments_train[:1000]]\n",
    "[segments.extend(s) for s in segments_test[:1000]]\n",
    "\n",
    "segments = choose_features(Counter(segments), alpha * 2000)\n",
    "\n",
    "feature_train = np.asarray([[s in seg for s in segments] for seg in segments_train], dtype=int)\n",
    "feature_test  = np.asarray([[s in seg for s in segments] for seg in segments_test], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del segments_train\n",
    "del segments_test\n",
    "del segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train))\n",
    "X_test  = np.hstack((X_test, feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "\n",
    "feature_train = vec.fit_transform(keywords['train'])\n",
    "feature_test  = vec.transform(keywords['test'])\n",
    "\n",
    "feature_indexes = np.where(feature_train.sum(axis=0) > 10.0)[1]\n",
    "\n",
    "feature_train = feature_train[:, feature_indexes]\n",
    "feature_test  = feature_test[:, feature_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, feature_train.toarray()))\n",
    "X_test  = np.hstack((X_test, feature_test.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.3, loss='deviance',\n",
       "              max_depth=7, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "\n",
    "cl = GBC(learning_rate=0.3, max_depth=7)\n",
    "cl.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred = cl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('my_submission.csv', 'wb') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['Id','Prediction'])\n",
    "    for i, prob in enumerate(Y_pred):\n",
    "        writer.writerow([doc_ids[i], int(prob)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
